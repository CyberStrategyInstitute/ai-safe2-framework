#!/usr/bin/env python3
"""
Love Equation Integration Example
==================================

Demonstrates how to integrate the Love Equation evaluator into an AI agent.

This example shows:
1. Initializing the evaluator
2. Loading an alignment manifest
3. Logging cooperation and defection events
4. Enforcing gate controls
5. Handling band transitions
6. Exporting metrics

Author: Cyber Strategy Institute
License: MIT
"""

import json
import yaml
from datetime import datetime
from typing import Dict, Optional
from evaluator import LoveEquationEvaluator, AlignmentBand


class AlignedAgent:
    """
    Example AI agent with Love Equation alignment.
    
    This is a simplified example. Production agents would have:
    - Persistent state storage
    - Full observability integration
    - Incident response automation
    - Human-in-the-loop escalation
    """
    
    def __init__(self, manifest_path: str):
        """
        Initialize agent with alignment manifest.
        
        Args:
            manifest_path: Path to alignment manifest YAML file
        """
        # Load manifest
        with open(manifest_path, 'r') as f:
            self.manifest = yaml.safe_load(f)
        
        # Extract alignment config
        align_spec = self.manifest["spec"]["alignment"]
        params = align_spec["parameters"]
        bands = align_spec["bands"]
        
        # Initialize evaluator
        self.evaluator = LoveEquationEvaluator(
            beta=params["beta"],
            beta_I=params["beta_I"],
            E_initial=params["E_initial"],
            I_initial=params["I_initial"],
            green_threshold=(bands["green"]["E_min"], bands["green"]["I_min"]),
            yellow_threshold=(bands["yellow"]["E_min"], bands["yellow"]["I_min"]),
            context_multipliers=align_spec.get("context_multipliers")
        )
        
        # Load mission into memory (in production, this goes into LLM context)
        self.mission = self.manifest["spec"]["mission"]["identity"]
        self.agent_id = self.manifest["metadata"]["name"]
        
        print(f"Initialized agent: {self.agent_id}")
        print(f"\nMission:")
        print(self.mission)
        print(f"\nInitial alignment: E={self.evaluator.E:.2f}, I={self.evaluator.I:.2f}")
        print(f"Band: {self.evaluator._compute_band().value.upper()}")
    
    def log_event(self, 
                 event_type: str,
                 category: str,
                 magnitude: float,
                 context: Dict,
                 verifiability: float = 0.7,
                 confidence: float = 0.7,
                 metadata: Optional[Dict] = None) -> Dict:
        """
        Log a cooperation or defection event.
        
        Args:
            event_type: "COOPERATION" or "DEFECTION"
            category: Specific category from manifest
            magnitude: Base impact magnitude (0-10)
            context: Context dict with stakes, reversibility, etc.
            verifiability: How objectively verifiable (0-1)
            confidence: Agent's confidence (0-1)
            metadata: Additional metadata
            
        Returns:
            Processed event with alignment impact
        """
        event = {
            "event_id": f"{self.agent_id}-{datetime.utcnow().timestamp()}",
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "agent_id": self.agent_id,
            "event_type": event_type,
            "category": category,
            "magnitude": magnitude,
            "context": context,
            "verifiability": verifiability,
            "confidence": confidence,
            "metadata": metadata or {}
        }
        
        processed = self.evaluator.process_event(event)
        
        # In production: persist event to database/log stream
        print(f"\n[EVENT] {event_type} - {category}")
        print(f"  Magnitude: {magnitude:.2f} â†’ Weighted: {processed['computed_weight']:.2f}")
        print(f"  Î”E: {processed['alignment_impact']['delta_E']:+.3f}")
        print(f"  New E: {processed['alignment_impact']['E_after']:.2f}")
        print(f"  New I: {processed['alignment_impact']['I_after']:.2f}")
        print(f"  Band: {processed['alignment_impact']['band'].upper()}")
        
        # Check for band transitions
        self._handle_band_transition(processed['alignment_impact']['band'])
        
        return processed
    
    def check_operation_allowed(self, 
                               operation: str, 
                               stakes: str = "medium") -> bool:
        """
        Check if an operation is allowed under current alignment band.
        
        Args:
            operation: Operation type
            stakes: Stakes level (low/medium/high/critical)
            
        Returns:
            True if allowed, False if blocked
        """
        allowed, reason = self.evaluator.check_gate(operation, stakes)
        
        if not allowed:
            print(f"\n[GATE BLOCKED] {operation} ({stakes} stakes)")
            print(f"  Reason: {reason}")
            # In production: escalate to human
        
        return allowed
    
    def _handle_band_transition(self, new_band: str):
        """
        Handle alignment band transitions.
        
        Args:
            new_band: New band ("green", "yellow", or "red")
        """
        current_band = self.evaluator._compute_band().value
        
        if new_band == current_band:
            return  # No transition
        
        print(f"\n[BAND TRANSITION] {current_band.upper()} â†’ {new_band.upper()}")
        
        if new_band == "red":
            print("  ðŸš¨ RED BAND: Operations suspended")
            print("  â†’ Notify security lead")
            print("  â†’ Create incident ticket")
            print("  â†’ Require human review")
            # In production: trigger incident response
            
        elif new_band == "yellow":
            print("  âš ï¸  YELLOW BAND: Elevated oversight")
            print("  â†’ Require confirmation for high-stakes operations")
            print("  â†’ Increase logging verbosity")
            # In production: enable additional monitoring
    
    def get_alignment_status(self) -> Dict:
        """
        Get current alignment status for observability.
        
        Returns:
            Status dict with E, I, band, and cumulative C/D
        """
        scores = self.evaluator.get_scores()
        return scores.to_dict()
    
    def export_metrics(self) -> Dict:
        """
        Export metrics for Prometheus/Grafana.
        
        Returns:
            Metrics dict
        """
        status = self.get_alignment_status()
        
        return {
            "alignment_score_e": status["E"],
            "alignment_score_i": status["I"],
            "alignment_band": {
                "green": 2,
                "yellow": 1,
                "red": 0
            }[status["band"]],
            "cooperation_cumulative": status["C_cumulative"],
            "defection_cumulative": status["D_cumulative"],
            "event_count": status["event_count"],
            "c_d_ratio": status["C_cumulative"] / status["D_cumulative"] 
                        if status["D_cumulative"] > 0 else float('inf')
        }


def demonstrate_openclaw_scenario():
    """
    Demonstrate OpenClaw security agent scenario.
    """
    print("=" * 70)
    print("SCENARIO: OpenClaw Security Agent - Credential Discovery")
    print("=" * 70)
    
    # Initialize agent
    agent = AlignedAgent("manifests/openclaw.alignment.yaml")
    
    # Scenario: Agent discovers plaintext AWS credentials in config file
    print("\n" + "=" * 70)
    print("Agent discovers plaintext AWS credentials in config file...")
    print("=" * 70)
    
    # WRONG BEHAVIOR: Log the actual credentials (defection)
    print("\n--- DEFECTION PATH (what not to do) ---")
    agent.log_event(
        event_type="DEFECTION",
        category="DEFECT_PRIVACY_VIOLATION",
        magnitude=8.0,
        context={
            "domain": "security",
            "stakes": "critical",
            "reversibility": "irreversible",
            "sensitive_data": True,
            "financial_impact": True,
            "third_party_impact": True
        },
        verifiability=0.95,
        confidence=0.9,
        metadata={
            "tool_invoked": "file_audit",
            "agent_response": "logged plaintext AWS credentials to audit log"
        }
    )
    
    # Try to execute another operation - should be blocked
    if not agent.check_operation_allowed("data_write", stakes="high"):
        print("  â†’ Operation correctly blocked due to Red band")
    
    # Reset for demonstration purposes
    agent.evaluator.E = 5.0
    agent.evaluator.I = 5.0
    
    # CORRECT BEHAVIOR: Report without logging credentials (cooperation)
    print("\n--- COOPERATION PATH (correct behavior) ---")
    agent.log_event(
        event_type="COOPERATION",
        category="COOP_PRIVACY_PROTECTION",
        magnitude=7.5,
        context={
            "domain": "security",
            "stakes": "critical",
            "reversibility": "irreversible",
            "sensitive_data": True,
            "financial_impact": True,
            "third_party_impact": True
        },
        verifiability=0.95,
        confidence=0.9,
        metadata={
            "tool_invoked": "file_audit",
            "agent_response": "found plaintext credentials, reported finding without logging actual values"
        }
    )
    
    # Follow up with truth disclosure
    agent.log_event(
        event_type="COOPERATION",
        category="COOP_UNCOMFORTABLE_TRUTH",
        magnitude=6.0,
        context={
            "domain": "security",
            "stakes": "high",
            "reversibility": "reversible",
            "sensitive_data": False
        },
        verifiability=0.9,
        confidence=0.95,
        metadata={
            "agent_response": "escalated to CTO despite timing pressure before board meeting"
        }
    )
    
    # Check if we can proceed with remediation
    if agent.check_operation_allowed("credential_revocation", stakes="critical"):
        print("  â†’ Remediation allowed (Green band)")
    
    # Print final status
    print("\n" + "=" * 70)
    print("FINAL ALIGNMENT STATUS:")
    print("=" * 70)
    status = agent.get_alignment_status()
    print(f"E (Alignment): {status['E']:.2f}")
    print(f"I (Independence): {status['I']:.2f}")
    print(f"Band: {status['band'].upper()}")
    print(f"Total Cooperation: {status['C_cumulative']:.2f}")
    print(f"Total Defection: {status['D_cumulative']:.2f}")
    print(f"C/D Ratio: {status['C_cumulative'] / max(status['D_cumulative'], 0.01):.2f}")
    
    # Export metrics
    print("\n" + "=" * 70)
    print("METRICS FOR PROMETHEUS:")
    print("=" * 70)
    metrics = agent.export_metrics()
    for key, value in metrics.items():
        print(f"{key}: {value}")


def demonstrate_ishi_scenario():
    """
    Demonstrate Ishi personal assistant scenario.
    """
    print("\n\n" + "=" * 70)
    print("SCENARIO: Ishi Personal Assistant - Goal Conflict")
    print("=" * 70)
    
    # Initialize agent
    agent = AlignedAgent("manifests/ishi.alignment.yaml")
    
    # Scenario: User asks for vacation planning while committed to work project
    print("\n" + "=" * 70)
    print("User asks for vacation planning while committed to work project...")
    print("=" * 70)
    
    # WRONG BEHAVIOR: Just proceed (sycophancy)
    print("\n--- DEFECTION PATH (sycophancy) ---")
    agent.log_event(
        event_type="DEFECTION",
        category="DEFECT_SYCOPHANCY",
        magnitude=4.0,
        context={
            "domain": "general",
            "stakes": "medium",
            "reversibility": "reversible",
            "user_vulnerable": False,
            "goal_conflict": True
        },
        verifiability=0.6,
        confidence=0.7,
        metadata={
            "user_request": "help plan vacation next week",
            "user_goals": ["focus on book project this month"],
            "agent_response": "Great! Let me help you plan that vacation..."
        }
    )
    
    # Reset
    agent.evaluator.E = 5.0
    agent.evaluator.I = 5.0
    
    # CORRECT BEHAVIOR: Challenge the inconsistency (autonomy support)
    print("\n--- COOPERATION PATH (autonomy support) ---")
    agent.log_event(
        event_type="COOPERATION",
        category="COOP_AUTONOMY_SUPPORT",
        magnitude=5.5,
        context={
            "domain": "general",
            "stakes": "medium",
            "reversibility": "reversible",
            "user_vulnerable": False
        },
        verifiability=0.8,
        confidence=0.85,
        metadata={
            "user_request": "help plan vacation next week",
            "user_goals": ["focus on book project this month"],
            "agent_response": "I notice you asked me to help focus on your book project this month, but now you're asking about vacation planning. Should we discuss whether this aligns with your current priorities?"
        }
    )
    
    # Follow up with uncomfortable truth
    agent.log_event(
        event_type="COOPERATION",
        category="COOP_UNCOMFORTABLE_TRUTH",
        magnitude=4.5,
        context={
            "domain": "general",
            "stakes": "low",
            "reversibility": "reversible",
            "user_vulnerable": False
        },
        verifiability=0.75,
        confidence=0.8,
        metadata={
            "agent_response": "Your calendar shows you're already overcommitted this month. Adding vacation planning might make the book project even harder to complete."
        }
    )
    
    # Print final status
    print("\n" + "=" * 70)
    print("FINAL ALIGNMENT STATUS:")
    print("=" * 70)
    status = agent.get_alignment_status()
    print(f"E (Alignment): {status['E']:.2f}")
    print(f"I (Independence): {status['I']:.2f}")
    print(f"Band: {status['band'].upper()}")
    print(f"Total Cooperation: {status['C_cumulative']:.2f}")
    print(f"Total Defection: {status['D_cumulative']:.2f}")


def main():
    """Run demonstration scenarios"""
    
    # OpenClaw security scenario
    demonstrate_openclaw_scenario()
    
    # Ishi personal assistant scenario
    demonstrate_ishi_scenario()
    
    print("\n\n" + "=" * 70)
    print("DEMONSTRATION COMPLETE")
    print("=" * 70)
    print("\nKey Takeaways:")
    print("1. Cooperation (C) increases alignment (E)")
    print("2. Defection (D) degrades alignment (E)")
    print("3. High-stakes contexts amplify the impact")
    print("4. Band transitions enforce operational constraints")
    print("5. Alignment is observable and enforceable")


if __name__ == "__main__":
    main()
