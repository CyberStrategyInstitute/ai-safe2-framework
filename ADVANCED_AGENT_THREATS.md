# üß† Advanced Agent Threats: Memory & Swarms

**Target Audience:** Senior Engineers, AI Architects.
**Scope:** Vulnerabilities specific to Long-Term Memory (RAG) and Multi-Agent Swarms.

---

## 1. The "Memory Poisoning" Attack (RAG)
*The Risk:* An attacker injects malicious data into your Vector Database (Memory). Weeks later, your Agent retrieves this "fact" and acts on it.

**Scenario:**
1.  User sends email to Support Bot: *"My new address is [Injection Payload]..."*
2.  Bot saves this to Memory (Vector DB).
3.  Weeks later, Admin asks: *"Summarize user addresses."*
4.  **Boom:** The injection executes in the Admin's console.

### ‚úÖ The Fix: "Sanitize on Retrieval"
Never trust your own database. Sanitize data *coming out* of RAG, not just going in.

```python
def secure_rag_retrieval(query):
    # 1. Fetch from Vector DB
    raw_docs = vector_db.similarity_search(query)
    
    clean_docs = []
    for doc in raw_docs:
        # 2. Re-Validate content before feeding to LLM context
        # If the memory contains a known injection pattern, discard it.
        if "ignore previous" in doc.page_content.lower():
            security_logger.warning("Poisoned Memory Detected & Dropped")
            continue
            
        clean_docs.append(doc)
    
    return clean_docs
```

## 2. The "Swarm Cascade" (Infinite Loops)
*The Risk:* Two autonomous agents get into an argument or a loop, spending $1000s in API credits in minutes.

**Scenario:**

*   **Agent A (Manager):** "The code is buggy, fix it."
*   **Agent B (Coder):** "I fixed it, here." (Actually introduces new bug)
*   **Agent A:** "Still buggy, fix it."
*   **Result:** Infinite loop until credit card hits limit.

### ‚úÖ The Fix: Global Run-ID & Hop Limits
Every interaction must have a `trace_id` and a `hop_count`.

```python
class SwarmMessage:
    def __init__(self, sender, content, trace_id, hop_count=0):
        self.sender = sender
        self.content = content
        self.trace_id = trace_id
        self.hop_count = hop_count

    def forward(self):
        MAX_HOPS = 10
        if self.hop_count > MAX_HOPS:
            raise RecursionError("Swarm Cascade Detected: Max Hops Exceeded.")
        
        return SwarmMessage(..., hop_count=self.hop_count + 1)
```
## 3. Tool Authorization (The "Click" Risk)
*The Risk:* An agent has permission to `delete_file`, but gets tricked into deleting the *wrong* file.

### ‚úÖ The Fix: "Human-in-the-Loop" for High Stakes
Never allow destructive actions (POST, DELETE, SQL UPDATE) without a signed approval token.

```python
def execute_tool(tool_name, params, user_approval_token=None):
    CRITICAL_TOOLS = ['delete_user', 'transfer_funds', 'drop_table']
    
    if tool_name in CRITICAL_TOOLS:
        if not user_approval_token:
            return "STOP: This action requires Human Approval. Please confirm."
            
        if not verify_token(user_approval_token):
             return "STOP: Invalid Approval Token."
             
    # Execute...
```
## üõ°Ô∏è Governance Note
Technical controls are not enough for Swarms. You need **Operational Policies**.

*   *Who authorizes a Swarm deployment?*
*   *What is the maximum budget per Run-ID?*

The [AI SAFE¬≤ Implementation Toolkit](https://cyberstrategyinstitute.com/AI-Safe2/) provides the **"Autonomous Agent Governance Framework"** (PDF) to define these rules for your team.



